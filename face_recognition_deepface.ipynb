{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Identificação do Problema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O problema abordado pelo projeto é o de identificação facial, ou seja, identificar indivíduos a partir de uma foto. Nesse contexto, o objetivo é não apenas identificar os rostos presentes na imagem, mas fazer  reconhecimento de indivíduos pré-cadastrados e a estimativa de caracaterísticas deles como: Gênero, Idade, Emoção e Raça. Com isso em mente, é interessante gerar uma plataforma web em que um usuário qualquer pode acessar e fazer upload de uma foto, a foto será analisada e retornada para o usuário com a identificação dos rostos, características e invidíuos, após isso, podem ser feitas edições adicionais na foto e ela pode ser salva pelo usuário em seu computador."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Pré-Processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparação do Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os dados de treinamento para o YOLO precisam estar em um modelo especifico para o funcionamento. Portanto é necessário separar os dados em uma pasta de treino e validação, em que ambos devem estar uma uma pasta de imagens, além disso é necessário ter uma pasta de 'labels' com as mesmas pastas de treino e validação com as informações de classes e 'caixas' a serem treinadas pelo modelo.\n",
    "\n",
    "Modelo diretório:\n",
    "\n",
    "data/\n",
    "\n",
    "----images/\n",
    "\n",
    "--------train/\n",
    "\n",
    "--------val/\n",
    "\n",
    "----labels/\n",
    "\n",
    "--------train/\n",
    "        \n",
    "--------val/    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset: https://www.kaggle.com/datasets/sbaghbidi/human-faces-object-detection?select=faces.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Criação do arquivo de labels\n",
    "\n",
    "# Criação dos parâmetros de X e Y centrais, além da altura e comprimento da caixa\n",
    "df = pd.read_csv('faces.csv')\n",
    "df['Xc'] = (df['x1'] + df['x0'])/2\n",
    "df['Yc'] = (df['y1'] + df['y0'])/2\n",
    "df['Box_width'] = df['x1'] - df['x0']\n",
    "df['Box_height'] = df['y1'] - df['y0']\n",
    "\n",
    "# Parâmenros de X e Y centrais, além da altura e comprimento da caixa\n",
    "#  como proporção das dimensões da imagem\n",
    "df_final = pd.DataFrame()\n",
    "df_final['img'] = df['image_name']\n",
    "df_final['class'] = 0\n",
    "df_final['Xc'] = df['Xc']/df['width']\n",
    "df_final['Yc'] = df['Yc']/df['height']\n",
    "df_final['height'] = df['Box_height']/df['height']\n",
    "df_final['width'] = df['Box_width']/df['width']\n",
    "\n",
    "\n",
    "# Separação dos dados de treino\n",
    "train = df_final.loc[df_final['img'].apply(lambda x: int(x[:-4])) > 480]\n",
    "\n",
    "for i, row in train.iterrows():\n",
    "    clas = round(row['class'], 4)\n",
    "    xc = round(row['Xc'], 4)\n",
    "    yc =round(row['Yc'], 4)\n",
    "    height = round(row['height'], 4)\n",
    "    width = round(row['width'], 4)\n",
    "\n",
    "    f = open(f\"train/{row['img'][:-4]}.txt\", \"a\")\n",
    "    f.write(f'{clas} {xc} {yc} {height} {width}\\n')\n",
    "    f.close()\n",
    "\n",
    "# Separação dos dados de validação\n",
    "val = df_final.loc[df_final['img'].apply(lambda x: int(x[:-4])) <= 480]\n",
    "\n",
    "for i, row in val.iterrows():\n",
    "    clas = round(row['class'], 4)\n",
    "    xc = round(row['Xc'], 4)\n",
    "    yc =round(row['Yc'], 4)\n",
    "    height = round(row['height'], 4)\n",
    "    width = round(row['width'], 4)\n",
    "\n",
    "    f = open(f\"val/{row['img'][:-4]}.txt\", \"a\")\n",
    "    f.write(f'{clas} {xc} {yc} {height} {width}\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Além disso, é necessário criar um arquivo .yaml com as informações sobre o dataset, os labels, e as classes. \n",
    "\n",
    "O seguinte modelo foi usado:\n",
    "\n",
    "path: \"./data\"\n",
    "\n",
    "train: images/train\n",
    "\n",
    "val: images/val\n",
    "\n",
    "nc: 1\n",
    "\n",
    "names: ['Face']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Extração de Padrões "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecção Facial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O modelo será treinado diversas vezes para que tenha uma boa precisão na construção da 'caixa'.\n",
    "\n",
    "O melhor modelo gerado ficará no caminho 'runs\\detect\\FacialDetection\\weights\\best.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "model = YOLO('yolov8n.pt')\n",
    "\n",
    "# Train the model\n",
    "model.train(data='face_detection.yaml', name='FacialDetection', seed=42, epochs=100, patience=40, rect=True, optimizer='Adam')\n",
    "valid_results = model.val()\n",
    "print(valid_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconhecimento Facial e extração de características"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para o reconhecimento de características das pessoas e identificação de indivíduos, foi utilizado o modelo pré treinado DeepFace. Para o reconhecimento de indivíduos o modelo funciona gerando embeddings dos indivíduos a partir de imagens de cadastro, e compara os embeddings de novas imagens com as do seu banco de dados. Dessa forma, se uma função de semelhança retornar um valor maior que um threshold pré determinado, ele considera que houve identificação do indivíduo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23-12-20 19:50:53 - ⚠️ Representations for images in ./db_rostos folder were previously stored in representations_vgg_face.pkl. If you added new instances after the creation, then please delete this file and call find function again. It will create it again.\n",
      "23-12-20 19:50:53 - There are 2 representations found in representations_vgg_face.pkl\n",
      "23-12-20 19:50:54 - find function lasts 1.1697442531585693 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Eduardo'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deepface import DeepFace\n",
    "person = DeepFace.find('./Fotos/Eduardo.jpg', db_path = \"./db_rostos\")\n",
    "person[0].identity[0].split('/')[2].split('.')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Pós Processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função visa detectar inicialmente os rostos da imagem passada, com isso, para cada rosto, fazer o reconhecimento e extração de características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "from deepface import DeepFace\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "model = YOLO('best.pt')\n",
    "\n",
    "def process_image(image):\n",
    "    # Detecção Facial\n",
    "    results = model.predict(image)\n",
    "    # Posições dos rostos\n",
    "    box = results[0].boxes.xyxy.tolist()\n",
    "\n",
    "    for face in box:\n",
    "        try:\n",
    "            # Pontos XY\n",
    "            x0 = int(face[0])\n",
    "            y0 = int(face[1])\n",
    "            x1 = int(face[2])\n",
    "            y1 = int(face[3])\n",
    "\n",
    "            # Recorte do rosto\n",
    "            individual_face = image[y0:y1, x0:x1]\n",
    "\n",
    "            # Dados sobre o rosto\n",
    "                # Reconhecimento Facial\n",
    "            person = DeepFace.find(individual_face, db_path = \"./db_rostos\")\n",
    "            per = 'N/A' if len(person[0]) == 0 else person[0].identity[0].split('/')[2].split('.')[0]\n",
    "            per = per.lower()\n",
    "                # Extração de características\n",
    "            res = DeepFace.analyze(individual_face)[0]\n",
    "            gender = res[\"dominant_gender\"].lower()\n",
    "            age = str(res[\"age\"]).lower()\n",
    "            emotion = res[\"dominant_emotion\"].lower()\n",
    "            race = res[\"dominant_race\"].lower()\n",
    "\n",
    "            # Inicio e Fim do rosto\n",
    "            start_point = (x0, y0)\n",
    "            end_point = (x1, y1)\n",
    "\n",
    "            # Inicio e fim dos textos\n",
    "            start_space = (x1, y0)\n",
    "                # Final do texto seguirá o tamanho do maior texto escrito\n",
    "            end_space = (x1+32+max(map(lambda x: cv2.getTextSize(x,cv2.FONT_HERSHEY_PLAIN,1, 1)[0][0], [race, emotion])), y0+105)\n",
    "\n",
    "            # Cor e espessura do retrato\n",
    "            color = (172, 52, 52)\n",
    "            thickness = 2\n",
    "\n",
    "            # Retrato na foto original e espaco para o texto\n",
    "            image = cv2.rectangle(image, start_point, end_point, color, thickness)\n",
    "            image = cv2.rectangle(image, start_space, end_space, color, -1) \n",
    "\n",
    "            # Resultados do rosto encontrado\n",
    "            image = cv2.putText(image, f'P: {per}', (x1+5, y0+18), cv2.FONT_HERSHEY_PLAIN, 1, (255, 255, 255), 1) \n",
    "            image = cv2.putText(image, f'G: {gender}', (x1+5, y0+38), cv2.FONT_HERSHEY_PLAIN, 1, (255, 255, 255), 1) \n",
    "            image = cv2.putText(image, f'A: {age}', (x1+5, y0+58), cv2.FONT_HERSHEY_PLAIN, 1, (255, 255, 255), 1) \n",
    "            image = cv2.putText(image, f'E: {emotion}', (x1+5, y0+78), cv2.FONT_HERSHEY_PLAIN, 1, (255, 255, 255), 1) \n",
    "            image = cv2.putText(image, f'R: {race}', (x1+5, y0+98), cv2.FONT_HERSHEY_PLAIN, 1, (255, 255, 255), 1) \n",
    "        except:\n",
    "\n",
    "            start_point = (x0, y0)\n",
    "            end_point = (x1, y1)\n",
    "            color = (172, 52, 52)\n",
    "            thickness = 2\n",
    "            image = cv2.rectangle(image, start_point, end_point, color, thickness)\n",
    "            \n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Uso do conhecimento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De forma a utilizar o conhecimento gerado pela agregação dos dois modelos, foi criada uma plataforma web simples com uso da biblioteca Dash, do python. Nela, o usuário faz upload da foto que deseja analisar, o modelo analisa a foto e ela é mostrada de volta para o usuário, que pode fazer edições para circular pessoas não encontradas pelo modelo ou ressaltar uma parte da imagem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x145731c1110>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import dash\n",
    "import numpy as np\n",
    "import dash_html_components as html\n",
    "from dash_canvas import DashCanvas\n",
    "from dash import dcc\n",
    "from dash.dependencies import Input, Output, State\n",
    "from dash_canvas.utils import array_to_data_url, image_string_to_PILImage\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "app = dash.Dash(__name__)\n",
    "\n",
    "canvas_width = 650\n",
    "\n",
    "# Layout da página html\n",
    "app.layout = html.Div([\n",
    "    html.H2('Reconhecimento facial e análise de características', style={'text-align':'center'}),\n",
    "\n",
    "    dcc.Upload(\n",
    "        id='upload-image',\n",
    "        children=html.Div([\n",
    "            'Arraste e solte ou ',\n",
    "            html.A('selecione uma imagem para analisar')\n",
    "        ]),\n",
    "        style={\n",
    "            'width': '99%',\n",
    "            'height': '60px',\n",
    "            'lineHeight': '60px',\n",
    "            'borderWidth': '1px',\n",
    "            'borderStyle': 'dashed',\n",
    "            'borderRadius': '5px',\n",
    "            'textAlign': 'center',\n",
    "            'margin': '10px'\n",
    "        },\n",
    "        multiple=True\n",
    "    ),\n",
    "    html.Div(id='output-image-upload',style={'width': '100%', 'display': 'flex', 'align-items':'center', 'justify-content':'center'})\n",
    "])\n",
    "\n",
    "# Funcao para aplicar o modelo na imagem e exibi-la ao usuario\n",
    "def parse_contents(contents, filename, date):\n",
    "    img = image_string_to_PILImage(contents)\n",
    "    img.save('img.jpg')\n",
    "    img = cv2.imread(\"img.jpg\")\n",
    "    img = process_image(img)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    pix = np.array(img)\n",
    "    img_content = array_to_data_url(pix)\n",
    "    return html.Div([DashCanvas(id='canvaas_image',\n",
    "                                image_content=img_content,\n",
    "                                lineWidth=5,\n",
    "                                lineColor='red',\n",
    "                                width=canvas_width)])\n",
    "\n",
    "\n",
    "# Funcao para reconhecer o upload de uma imagem e chamar a funcao acima\n",
    "@app.callback(Output('output-image-upload', 'children'),\n",
    "              Input('upload-image', 'contents'),\n",
    "              State('upload-image', 'filename'),\n",
    "              State('upload-image', 'last_modified'))\n",
    "def update_output(list_of_contents, list_of_names, list_of_dates):\n",
    "    if list_of_contents is not None:\n",
    "        children = [\n",
    "            parse_contents(c, n, d) for c, n, d in\n",
    "            zip(list_of_contents, list_of_names, list_of_dates)]\n",
    "        return children\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
