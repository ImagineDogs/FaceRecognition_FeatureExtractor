{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecção Facial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento do Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparação do Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os dados de treinamento para o YOLO precisam estar em um modelo especifico para o funcionamento. Portanto é necessário separar os dados em uma pasta de treino e validação, em que ambos devem estar uma uma pasta de imagens, além disso é necessário ter uma pasta de 'labels' com as mesmas pastas de treino e validação com as informações de classes e 'caixas' a serem treinadas pelo modelo.\n",
    "\n",
    "Modelo diretório:\n",
    "\n",
    "data/\n",
    "\n",
    "----images/\n",
    "\n",
    "--------train/\n",
    "\n",
    "--------val/\n",
    "\n",
    "----labels/\n",
    "\n",
    "--------train/\n",
    "        \n",
    "--------val/    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Criação do arquivo de labels\n",
    "\n",
    "# Criação dos parâmetros de X e Y centrais, além da altura e comprimento da caixa\n",
    "df = pd.read_csv('faces.csv')\n",
    "df['Xc'] = (df['x1'] + df['x0'])/2\n",
    "df['Yc'] = (df['y1'] + df['y0'])/2\n",
    "df['Box_width'] = df['x1'] - df['x0']\n",
    "df['Box_height'] = df['y1'] - df['y0']\n",
    "\n",
    "# Parâmenros de X e Y centrais, além da altura e comprimento da caixa\n",
    "#  como proporção das dimensões da imagem\n",
    "df_final = pd.DataFrame()\n",
    "df_final['img'] = df['image_name']\n",
    "df_final['class'] = 0\n",
    "df_final['Xc'] = df['Xc']/df['width']\n",
    "df_final['Yc'] = df['Yc']/df['height']\n",
    "df_final['height'] = df['Box_height']/df['height']\n",
    "df_final['width'] = df['Box_width']/df['width']\n",
    "\n",
    "\n",
    "# Separação dos dados de treino\n",
    "train = df_final.loc[df_final['img'].apply(lambda x: int(x[:-4])) > 480]\n",
    "\n",
    "for i, row in train.iterrows():\n",
    "    clas = round(row['class'], 4)\n",
    "    xc = round(row['Xc'], 4)\n",
    "    yc =round(row['Yc'], 4)\n",
    "    height = round(row['height'], 4)\n",
    "    width = round(row['width'], 4)\n",
    "\n",
    "    f = open(f\"train/{row['img'][:-4]}.txt\", \"a\")\n",
    "    f.write(f'{clas} {xc} {yc} {height} {width}\\n')\n",
    "    f.close()\n",
    "\n",
    "# Separação dos dados de validação\n",
    "val = df_final.loc[df_final['img'].apply(lambda x: int(x[:-4])) <= 480]\n",
    "\n",
    "for i, row in val.iterrows():\n",
    "    clas = round(row['class'], 4)\n",
    "    xc = round(row['Xc'], 4)\n",
    "    yc =round(row['Yc'], 4)\n",
    "    height = round(row['height'], 4)\n",
    "    width = round(row['width'], 4)\n",
    "\n",
    "    f = open(f\"val/{row['img'][:-4]}.txt\", \"a\")\n",
    "    f.write(f'{clas} {xc} {yc} {height} {width}\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Além disso, é necessário criar um arquivo .yaml com as informações sobre o dataset, os labels, e as classes. \n",
    "\n",
    "O seguinte modelo foi usado:\n",
    "\n",
    "path: \"./data\"\n",
    "\n",
    "train: images/train\n",
    "\n",
    "val: images/val\n",
    "\n",
    "nc: 1\n",
    "\n",
    "names: ['Face']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O modelo será treinado diversas vezes para que tenha uma boa precisão na construção da 'caixa'.\n",
    "\n",
    "O melhor modelo gerado ficará no caminho 'runs\\detect\\FacialDetection\\weights\\best.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "model = YOLO('yolov8n.pt')\n",
    "\n",
    "# Train the model\n",
    "model.train(data='face_detection.yaml', name='FacialDetection', seed=42, epochs=100, patience=40, rect=True, optimizer='Adam')\n",
    "valid_results = model.val()\n",
    "print(valid_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Função para detecção e reconhecimento facial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "from deepface import DeepFace\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "model = YOLO('best.pt')\n",
    "\n",
    "def process_image(image):\n",
    "    # Detecção Facial\n",
    "    results = model.predict(image)\n",
    "    # Posições dos rostos\n",
    "    box = results[0].boxes.xyxy.tolist()\n",
    "\n",
    "    for face in box:\n",
    "        try:\n",
    "            # Pontos XY\n",
    "            x0 = int(face[0])\n",
    "            y0 = int(face[1])\n",
    "            x1 = int(face[2])\n",
    "            y1 = int(face[3])\n",
    "\n",
    "            # Recorte do rosto\n",
    "            individual_face = image[y0:y1, x0:x1]\n",
    "\n",
    "            # Dados sobre o rosto\n",
    "                # Reconhecimento Facial\n",
    "            person = DeepFace.find(individual_face, db_path = \"./db_rostos\")\n",
    "            per = 'N/A' if len(person[0]) == 0 else person[0].identity[0].split('/')[2].split('.')[0]\n",
    "            per = per.lower()\n",
    "                # Extração de características\n",
    "            res = DeepFace.analyze(individual_face)[0]\n",
    "            gender = res[\"dominant_gender\"].lower()\n",
    "            age = str(res[\"age\"]).lower()\n",
    "            emotion = res[\"dominant_emotion\"].lower()\n",
    "            race = res[\"dominant_race\"].lower()\n",
    "\n",
    "            # Inicio e Fim do rosto\n",
    "            start_point = (x0, y0)\n",
    "            end_point = (x1, y1)\n",
    "\n",
    "            # Inicio e fim dos textos\n",
    "            start_space = (x1, y0)\n",
    "                # Final do texto seguirá o tamanho do maior texto escrito\n",
    "            end_space = (x1+32+max(map(lambda x: cv2.getTextSize(x,cv2.FONT_HERSHEY_PLAIN,1, 1)[0][0], [race, emotion])), y0+105)\n",
    "\n",
    "            # Cor e espessura do retrato\n",
    "            color = (172, 52, 52)\n",
    "            thickness = 2\n",
    "\n",
    "            # Retrato na foto original e espaco para o texto\n",
    "            image = cv2.rectangle(image, start_point, end_point, color, thickness)\n",
    "            image = cv2.rectangle(image, start_space, end_space, color, -1) \n",
    "\n",
    "            # Resultados do rosto encontrado\n",
    "            image = cv2.putText(image, f'P: {per}', (x1+5, y0+18), cv2.FONT_HERSHEY_PLAIN, 1, (255, 255, 255), 1) \n",
    "            image = cv2.putText(image, f'G: {gender}', (x1+5, y0+38), cv2.FONT_HERSHEY_PLAIN, 1, (255, 255, 255), 1) \n",
    "            image = cv2.putText(image, f'A: {age}', (x1+5, y0+58), cv2.FONT_HERSHEY_PLAIN, 1, (255, 255, 255), 1) \n",
    "            image = cv2.putText(image, f'E: {emotion}', (x1+5, y0+78), cv2.FONT_HERSHEY_PLAIN, 1, (255, 255, 255), 1) \n",
    "            image = cv2.putText(image, f'R: {race}', (x1+5, y0+98), cv2.FONT_HERSHEY_PLAIN, 1, (255, 255, 255), 1) \n",
    "        except:\n",
    "            pass\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plataforma Web para uso do conhecimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x20e59dcca90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x480 5 Faces, 177.2ms\n",
      "Speed: 36.5ms preprocess, 177.2ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "23-12-17 20:40:44 - ⚠️ Representations for images in ./db_rostos folder were previously stored in representations_vgg_face.pkl. If you added new instances after the creation, then please delete this file and call find function again. It will create it again.\n",
      "23-12-17 20:40:44 - There are 2 representations found in representations_vgg_face.pkl\n",
      "23-12-17 20:40:44 - find function lasts 0.3456571102142334 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: race: 100%|██████████| 4/4 [00:00<00:00,  4.25it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23-12-17 20:40:45 - ⚠️ Representations for images in ./db_rostos folder were previously stored in representations_vgg_face.pkl. If you added new instances after the creation, then please delete this file and call find function again. It will create it again.\n",
      "23-12-17 20:40:45 - There are 2 representations found in representations_vgg_face.pkl\n",
      "23-12-17 20:40:45 - find function lasts 0.37296009063720703 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: race: 100%|██████████| 4/4 [00:00<00:00,  4.41it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23-12-17 20:40:46 - ⚠️ Representations for images in ./db_rostos folder were previously stored in representations_vgg_face.pkl. If you added new instances after the creation, then please delete this file and call find function again. It will create it again.\n",
      "23-12-17 20:40:46 - There are 2 representations found in representations_vgg_face.pkl\n",
      "23-12-17 20:40:47 - find function lasts 0.31015682220458984 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: race: 100%|██████████| 4/4 [00:00<00:00,  4.41it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23-12-17 20:40:48 - ⚠️ Representations for images in ./db_rostos folder were previously stored in representations_vgg_face.pkl. If you added new instances after the creation, then please delete this file and call find function again. It will create it again.\n",
      "23-12-17 20:40:48 - There are 2 representations found in representations_vgg_face.pkl\n",
      "23-12-17 20:40:48 - find function lasts 0.3055891990661621 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: race: 100%|██████████| 4/4 [00:00<00:00,  4.27it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23-12-17 20:40:49 - ⚠️ Representations for images in ./db_rostos folder were previously stored in representations_vgg_face.pkl. If you added new instances after the creation, then please delete this file and call find function again. It will create it again.\n",
      "23-12-17 20:40:49 - There are 2 representations found in representations_vgg_face.pkl\n"
     ]
    }
   ],
   "source": [
    "import dash\n",
    "import numpy as np\n",
    "import dash_html_components as html\n",
    "from dash_canvas import DashCanvas\n",
    "from dash import dcc\n",
    "from dash.dependencies import Input, Output, State\n",
    "from dash_canvas.utils import array_to_data_url, image_string_to_PILImage\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "app = dash.Dash(__name__)\n",
    "\n",
    "canvas_width = 650\n",
    "\n",
    "app.layout = html.Div([\n",
    "    html.H2('Reconhecimento facial e análise de características', style={'text-align':'center'}),\n",
    "\n",
    "\n",
    "    dcc.Upload(\n",
    "        id='upload-image',\n",
    "        children=html.Div([\n",
    "            'Arraste e solte ou ',\n",
    "            html.A('selecione uma imagem para analisar')\n",
    "        ]),\n",
    "        style={\n",
    "            'width': '99%',\n",
    "            'height': '60px',\n",
    "            'lineHeight': '60px',\n",
    "            'borderWidth': '1px',\n",
    "            'borderStyle': 'dashed',\n",
    "            'borderRadius': '5px',\n",
    "            'textAlign': 'center',\n",
    "            'margin': '10px'\n",
    "        },\n",
    "        # Allow multiple files to be uploaded\n",
    "        multiple=True\n",
    "    ),\n",
    "    html.Div(id='output-image-upload',style={'width': '100%', 'display': 'flex', 'align-items':'center', 'justify-content':'center'})\n",
    "])\n",
    "\n",
    "\n",
    "def parse_contents(contents, filename, date):\n",
    "    img = image_string_to_PILImage(contents)\n",
    "    img.save('img.jpg')\n",
    "    img = cv2.imread(\"img.jpg\")\n",
    "    img = process_image(img)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    pix = np.array(img)\n",
    "    img_content = array_to_data_url(pix)\n",
    "    return html.Div([DashCanvas(id='canvaas_image',\n",
    "                                image_content=img_content,\n",
    "                                lineWidth=5,\n",
    "                                lineColor='red',\n",
    "                                width=canvas_width)])\n",
    "\n",
    "\n",
    "@app.callback(Output('output-image-upload', 'children'),\n",
    "              Input('upload-image', 'contents'),\n",
    "              State('upload-image', 'filename'),\n",
    "              State('upload-image', 'last_modified'))\n",
    "def update_output(list_of_contents, list_of_names, list_of_dates):\n",
    "    if list_of_contents is not None:\n",
    "        children = [\n",
    "            parse_contents(c, n, d) for c, n, d in\n",
    "            zip(list_of_contents, list_of_names, list_of_dates)]\n",
    "        return children\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
